
\section{Lower bounds for desinty estimation in one- and two- block graphs}
\label{sec:lower}

\begin{remark} Our bounds apply not only to the particular measure of
  distance on distributions used in defining node-differential
  privacy, but to any measure of distance that implies that two
  distributions cannot be reliably distinguished. A randomized
  algorithm $A$ is $\epsilon$-node-TV-stable if for all graphs $G$ and
  $G'$ that differ by the rewiring of one vertex, the distributions of
  $A(G)$ and $A(G')$ are $\tfrac 1 3$-close in \emph{total variation
    distance} (or statistical difference). 
\end{remark}

\subsection{Lower bounds for $G(n,m)$} \label{sec:lower-one-block}

\begin{proposition}
  Let $n$ be sufficiently large, and $k = \tfrac 1 2 \sqrt{n \log n}$. Let $m = {\binom n 2} - \frac k 2$
  Let $P = G(n,m)$ and $Q = G(n,m +k )$.   
  There exists a coupling of $(G,H)$ of $P$ and $Q$ such that, with probability at least $9/10$, one can obtain $H$ from $G$ by rewiring one vertex.  
\end{proposition}

\begin{proof}
  To be written.
\end{proof}

\begin{theorem}
  There exists a constant $\epsilon_0>0$ such that no
  $\epsilon_0$-node DP private algorithm can distinguish $G(n,m)$ from
  $G(n,m +k )$, for $k = \Theta(\sqrt{n \log n})$ and $m = \binom n 2
  - \tfrac k 2$. In particular, the upper bound of
  Proposition~\ref{prop:32-fixed-m} is tight for constant $\epsilon$
  and $\rho$. 
\end{theorem}

% \begin{proof}
%   First, if $G(n,m)$ and
%   $G(n,m +k )$ are not reliably distinguishable by node-private
%   algorithms, then no node private algorithm can estimate $m$ to
%   within error $k/2$ with high probability. 
% \end{proof}

\subsection{Lower bounds for general graphons}
\label{sec:lower-one-block}

We show that one cannot achieve the $\max(\frac 1 n, \frac 1
{\epsilon n^{3/2}}$ rate in general graphons. Even for the simplest
non-constant graphons, those with 2 blocks of similar size,
node-private algorithms incur error $\frac1 {\epsilon n}$ when
estimating the density. 

\begin{theorem}
  There exists a distribution $P$ on bounded 2-block graphons such that 
  for all $\eps$-DP algorithms $A$, the error in estimating the
  density of $W$ (equivalently, the expected edge density of graphs
  drawn from $G_n(W)$) is at least $\Omega(\max (\frac{1}{\sqrt{n}},
  \frac 1 {\epsilon n}))$.
\end{theorem}

\begin{proof}
  Given a parameter $q\in [0,1]$, let $W_q:[0,1]^2\to [0,1]$ be the
  graphon given by 
  $$W_q(x,y) =
  \begin{cases}
    1 & \text{if } x, y \leq q \text{ or } x,y \geq q\, , \\
    0 & \text{otherwise.}
  \end{cases}$$
  This is a 2-block graphon with blocks of sizes $q$ and $1-q$,
  respectively. The graphs generated from $W_q$ consist of two
  cliques (of size roughly $qn$ and $(1-q)n$) so it is easy to know
  which vertices belong to the same block. 

  The density of $W_q$ is
  $q^2+ (1-q)^2 = \frac 1 2 + 2 (q-\frac 1 2)^2$. Consider an
  algorithm that, given $G\sim G_n(W)$, aims to estimate the density
  of $W_q$. We can use its output $\hat \rho$ to estimate $q$ by
  setting $\hat q = \frac 1 2 - \sqrt{\frac{\hat \rho - \frac 1
      2}{2}}$. This function's derivative is finite and nonzero as
  long as $\hat \rho$ is bounded away from $\frac 1 2$. Thus, an
  algorithm that can estimate $\rho$ within error $\alpha$ on samples
  from $W_q$ can be used to estimate $q$ up to error $O(\alpha)$ (as
  long as $\hat \rho$ is bounded away from 1/2). 

  

  

\end{proof}

% \begin{claim}
%   For all real numbers $x$, we have
%   $$ (x+1) (x-1) = x^2 -1.$$
% \end{claim}

% \begin{proof}
%   \begin{eqnarray*}
%   (x+1) (x-1) 
%     & = & (x+1) x - (x+1) \\
%     & = & x^2 + x - x - 1 \\
%     & = & x^2 - 1 \\
%   \end{eqnarray*}
% \end{proof}

% For example, when $x=5$, the claim tells us that $6\times 4 = 5^2 -1$,
% which is $24$.








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "FOCS"
%%% End:
