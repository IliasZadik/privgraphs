
\section{Lower bounds for desinty estimation in one- and two- block graphs}
\label{sec:lower}

\begin{remark} Our bounds apply not only to the particular measure of
  distance on distributions used in defining node-differential
  privacy, but to any measure of distance that implies that two
  distributions cannot be reliably distinguished. A randomized
  algorithm $A$ is $\epsilon$-node-TV-stable if for all graphs $G$ and
  $G'$ that differ by the rewiring of one vertex, the distributions of
  $A(G)$ and $A(G')$ are $\tfrac 1 3$-close in \emph{total variation
    distance} (or statistical difference). 
\end{remark}

\subsection{Lower bounds for $G(n,m)$} \label{sec:lower-one-block}

\begin{proposition}
  Let $n$ be sufficiently large, and $k = \tfrac 1 2 \sqrt{n \log n}$. Let $m = {\binom n 2} - \frac k 2$
  Let $P = G(n,m)$ and $Q = G(n,m +k )$.   
  There exists a coupling of $(G,H)$ of $P$ and $Q$ such that, with probability at least $9/10$, one can obtain $H$ from $G$ by rewiring one vertex.  
\end{proposition}

\begin{proof}
  To be written.
\end{proof}

\begin{theorem}
  There exists a constant $c$ such that, for all $\epsilon< c$, no
  $\epsilon$-node DP private algorithm can distinguish $G(n,m)$ from
  $G(n,m +k )$, for $k = \Theta(\sqrt{n \log n})$ and $m = \binom n 2
  - \tfrac k 2$. In particular, the upper bound of
  Proposition~\ref{prop:32-fixed-m} is tight for constant $\epsilon$
  and $\rho$. 
\end{theorem}

\begin{proof}
  First, if $G(n,m)$ and
  $G(n,m +k )$ are not reliably distinguishable by node-private
  algorithms, then no node private algorithm can 

  
\end{proof}

\subsection{Lower bounds for general graphons}
\label{sec:lower-one-block}












%%% Local Variables:
%%% mode: latex
%%% TeX-master: "FOCS"
%%% End:
