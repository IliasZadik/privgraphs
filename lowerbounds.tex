
\section{Lower bounds for desinty estimation in one- and two- block graphs}
\label{sec:lower}

\begin{remark} Our bounds apply not only to the particular measure of
  distance on distributions used in defining node-differential
  privacy, but to any measure of distance that implies that two
  distributions cannot be reliably distinguished. A randomized
  algorithm $A$ is $\epsilon$-node-TV-stable if for all graphs $G$ and
  $G'$ that differ by the rewiring of one vertex, the distributions of
  $A(G)$ and $A(G')$ are $\tfrac 1 3$-close in \emph{total variation
    distance} (or statistical difference). 
\end{remark}

\subsection{Lower bounds for $G(n,m)$} \label{sec:lower-one-block}

\begin{proposition}
  Let $n$ be sufficiently large, and $k = \tfrac 1 2 \sqrt{n \log
    n}$. Let $m = {\binom n 2} - \frac k 2$ Let $P = G(n,m)$ and
  $Q = G(n,m +k )$.  There exists a coupling of $(G,H)$ of $P$ and $Q$
  such that, with probability at least $9/10$, one can obtain $H$ from
  $G$ by rewiring one vertex.
\end{proposition}

\begin{proof}
  To be written.
\end{proof}

\begin{theorem}
  There exists constants $\epsilon_0,\beta >0$ such that no
  $\epsilon_0$-node DP private algorithm can distinguish $G(n,m)$ from
  $G(n,m +k )$ with probability higher than $1-\beta$, for $k = \Theta(\sqrt{n \log n})$ and $m = \binom n 2
  - \tfrac k 2$.  In particular, the upper bound of
  Proposition~\ref{prop:32-fixed-m} is tight for constant $\epsilon$
  and $\rho$. 
\end{theorem}

% \begin{proof}
%   First, if $G(n,m)$ and
%   $G(n,m +k )$ are not reliably distinguishable by node-private
%   algorithms, then no node private algorithm can estimate $m$ to
%   within error $k/2$ with high probability. 
% \end{proof}

\subsection{Lower bounds for general graphons}
\label{sec:lower-one-block}

We show that one cannot achieve the $\max(\frac 1 n, \frac 1
{\epsilon n^{3/2}})$ rate in general graphons. Even for the simplest
non-constant graphons, those with 2 blocks of similar size,
node-private algorithms incur error $\frac1 {\epsilon n}$ when
estimating the density. 

\begin{theorem}
  There exists a distribution $P$ on bounded 2-block graphons such that 
  for all $\eps$-DP algorithms $A$ with $\eps\geq 1/n$, the error in estimating the
  density of $W$ (equivalently, the expected edge density of graphs
  drawn from $G_n(W)$) is at least $\Omega(\max (\frac{1}{\sqrt{n}},
  \frac 1 {\epsilon n}))$.
\end{theorem}

\begin{proof} We proceed by giving a reduction to (regular)
  differentially private estimation of the secret parameter $q$ given
  $n$ samples from a Bernouilli distribution (that is, $n$ biased
  coins, each of which is heads independently with probability $q$). 

  Given a parameter $q\in [0,1]$, let $W_q:[0,1]^2\to [0,1]$ be the
  graphon given by 
  $$W_q(x,y) =
  \begin{cases}
    1 & \text{if } x, y \leq q \text{ or } x,y \geq q\, , \\
    0 & \text{otherwise.}
  \end{cases}$$
  This is a 2-block graphon with blocks of sizes $q$ and $1-q$,
  respectively. The graphs generated from $W_q$ consist of two
  cliques (of size roughly $qn$ and $(1-q)n$) so it is easy to know
  which vertices belong to the same block. 

  The density of $W_q$ is $\tau(q)= 
  q^2+ (1-q)^2 = \frac 1 2 + 2 (q-\frac 1 2)^2$. Consider an
  algorithm $A$ that, given $G\sim G_n(W)$, aims to estimate the density $\tau(q)$
  of $W_q$. We can use its output (call it $\hat \tau$) to estimate $q$ by
  setting $\hat q = \frac 1 2 - \sqrt{\frac{\hat \tau - \frac 1
      2}{2}}$. This function's derivative is finite and nonzero as
  long as $\hat \tau$ is bounded away from $\frac 1 2$. Thus, an
  algorithm that can estimate $\tau(q)$ within error $\alpha$ on samples
  from $W_q$ can be used to estimate $q$ up to error $O(\alpha)$ (as
  long as $q$ is bounded away from 1/2). 

  To reduce to estimation of the Bernouilli parameter, suppose we are
  given a sample $X = (X_1,X_2,...,X_n)$ of size  $n$, drawn
  i.i.d. from Bernouilli($q$) for uknown $q$. We may generate a graph $G(X)$
  by creating two cliques of size $N_0$ and $N_1$, respectively, where
  $N_1 = \sum_i X_i$ (the number of ones in $X$) and $N_0 =
  n-N_1$. The distribution of $G(X)$ is exactly $G_n(W_q)$, and so we
  can run our density estimation algorithm $A$ to get an estimate $\hat
  \tau$ of the density of $W_q$ and use that to compute $\hat q$, an
  estimate of $q$. 

  Observe that, if $A$ is $\eps$-node-differentially private, then the
  composed algorithm
  $A(G(\cdot))$ is $\eps$-differentially private with respect to its
  input (from
  $ \{0,1\}^n$). To see why, note that changing one bit of the string $x$
  changes the edges of exactly one vertex in $G(x)$ (corresponding to
  a change in the clique to which it is assigned). Since $A$ is
  $\eps$-node-differentially private, a change in one bit of $x$
  yields a change of at most $\eps$ in the distribution of
  $A(G(x))$. 

 Fix a constant $c>0$ and consider the distribution $P$ obtained by choosing $q$ uniformly in
 ${\frac 1 4, \frac 14 +\alpha}$, where $\alpha = c\max (\frac{1}{\sqrt{n}},
  \frac 1 {\epsilon n})$ is the desired error bound, and outputing
  $W_q$. One can pick $c$ so that there is no $\eps$-differentially private
  algorithm that can distinguish the corresponging Bernouilli
  distributions (with $q \in {\frac 1 4, \frac 14 +\alpha}$) with
  probability better than $0.9$.

  An $\eps$-node-DP algorithm for estimating the density with error
  $o(\alpha)$ could be used to create an algorithm for Bernouilli
  estimation with error $o(\alpha)$, which would in turn allow one to
  estimate $q$ with error $o(\alpha)$, yielding a contradiction.
\end{proof}









%%% Local Variables:
%%% mode: latex
%%% TeX-master: "FOCS"
%%% End:
